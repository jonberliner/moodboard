{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6d706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweaked from https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel.forward.returns\n",
    "\n",
    "# below is standard CLIP usage to score text snippets against a photo\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "image1 = Image.open(requests.get(url, stream=True).raw)\n",
    "image2 = Image.open(requests.get(url, stream=True).raw)  # assume a different image\n",
    "\n",
    "images = [image1, image2]\n",
    "\n",
    "# the rest of this cell is standard use of CLIP\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0ebdd",
   "metadata": {},
   "source": [
    "We need to update the above so that we can calculate the scaled dot product scores between the images and images, rather than just between images and text.  below shows how to get the image embeddings.\n",
    "\n",
    "I believe I've done that below working off of the source code [here](https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/clip/modeling_clip.py#L1074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fa0ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff34415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for calculating similarity scores between images\n",
    "import torch\n",
    "\n",
    "image_embeds = outputs.image_embeds\n",
    "\n",
    "logit_scale = model.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059c4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume these are different sets of images\n",
    "\n",
    "moodboard_images = images\n",
    "\n",
    "recommendation_candidate_images = images\n",
    "\n",
    "# text is just here to make the thing run\n",
    "inputs1 = processor(text=[\"dummy\"], images=moodboard_images, return_tensors=\"pt\", padding=True)\n",
    "inputs2 = processor(text=[\"dummy\"], images=recommendation_candidate_images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "moodboard_image_embeds = model(**inputs1).image_embeds\n",
    "recommendation_candidate_image_embeds = model(**inputs2).image_embeds\n",
    "\n",
    "# hopefully, the closer these are, the higher the recommendation quality\n",
    "recommender_scores = torch.matmul(\n",
    "    recommendation_candidate_image_embeds,\n",
    "    moodboard_image_embeds.t()\n",
    ") * logit_scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
